{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure accuracy of agentic app with Arize and Langflow\n",
    "\n",
    "In this notebook we'll step through how to use open source tools [Arize](https://arize.com/) and [Langflow](https://www.langflow.org/) and to measure the accuracy of an agentic application.\n",
    "\n",
    "## Prepping a Dataset\n",
    "\n",
    "Let's start by gathering some example data from a standard question and answer benchmark. We'll use the Standford Question Answering Dataset (SQuAD).\n",
    "\n",
    "Available at [https://rajpurkar.github.io/SQuAD-explorer/](https://rajpurkar.github.io/SQuAD-explorer/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully\n",
      "Dataset loaded with 35 articles\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Download the file\n",
    "url = 'https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the download was successful\n",
    "if response.status_code == 200:\n",
    "    # Save the file locally\n",
    "    with open('dev-v2.0.json', 'w') as file:\n",
    "        json.dump(response.json(), file)\n",
    "    print(\"File downloaded successfully\")\n",
    "    \n",
    "    # Load the data\n",
    "    data = response.json()\n",
    "    print(f\"Dataset loaded with {len(data['data'])} articles\")\n",
    "else:\n",
    "    print(f\"Failed to download file. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all articles from the dataset\n",
    "docs = []\n",
    "for record in data[\"data\"]:\n",
    "    text = \"\"\n",
    "    for paragraph in record[\"paragraphs\"]:\n",
    "        text += paragraph[\"context\"] + \"\\n\"\n",
    "    docs.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of wikipedia articles in dataset: 35.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of wikipedia articles in dataset: {len(docs)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create markdown files for each article for use in Langflow UI\n",
    "for i, doc in enumerate(docs):\n",
    "    filename = f\"data/markdown_files/article_{i}.md\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all questions and answers for each article\n",
    "qandas = []\n",
    "for record in data[\"data\"]:\n",
    "    for paragraph in record[\"paragraphs\"]:\n",
    "        for qa in paragraph[\"qas\"]:\n",
    "            if len(qa[\"answers\"]) > 0:\n",
    "                qandas.append((qa[\"question\"], qa[\"answers\"][0][\"text\"]))\n",
    "\n",
    "# there are about 6000 Q&A pairs in this data set.  \n",
    "# Let's shorten to 100 to make it easier to run our tests\n",
    "import random\n",
    "samples = random.sample(qandas, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('What type of fossils were found in China?', 'Cambrian sessile frond-like fossil Stromatoveris')\n",
      "('Where do pharmacists acquire more preparation following pharmacy school?', 'a pharmacy practice residency')\n",
      "('What contributed to the decreased inequality between trained and untrained workers?', 'period of compression')\n"
     ]
    }
   ],
   "source": [
    "# Let's look at a couple examples of questions and answers\n",
    "print(samples[0])\n",
    "print(samples[1])\n",
    "print(samples[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install modules needed for the rest of this example\n",
    "\n",
    "We'll need the following modules to be available\n",
    "\n",
    "- `langflow` for rapidly designing AI workflows\n",
    "- `phoenix-arize` to run evals\n",
    "- `pandas` to format the data\n",
    "- `openai` for access to LLMs and embedding models\n",
    "\n",
    "If you don't already have these installed, go ahead and run the install commands below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: uv in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.5.15)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPython 3.11.7 interpreter at: \u001b[36m/opt/homebrew/opt/python@3.11/bin/python3.11\u001b[39m\n",
      "Creating virtual environment at: \u001b[36m.venv\u001b[39m\n",
      "Activate with: \u001b[32msource .venv/bin/activate\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "# uv requires a virtual environment to run\n",
    "!uv venv\n",
    "!source .venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install all the modules for the rest of the notebook\n",
    "!uv pip install langflow pandas openai arize -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our ground truth data to Arize\n",
    "\n",
    "Now that we have a set of questions and answers, we can use this to evaluate the quality of our agentic application.\n",
    "\n",
    "Let's upload this dataset to Phoenix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start by creating a dataframe with our Q&A pairs and save a copy\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(samples, columns=[\"question\", \"answer\"])\n",
    "\n",
    "# only run this line if you want to resave the dataframe\n",
    "df.to_csv(\"data/qa_pairs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What type of fossils were found in China?</td>\n",
       "      <td>Cambrian sessile frond-like fossil Stromatoveris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Where do pharmacists acquire more preparation following pharmacy school?</td>\n",
       "      <td>a pharmacy practice residency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What contributed to the decreased inequality between trained and untrained workers?</td>\n",
       "      <td>period of compression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                              question  \\\n",
       "0                                            What type of fossils were found in China?   \n",
       "1             Where do pharmacists acquire more preparation following pharmacy school?   \n",
       "2  What contributed to the decreased inequality between trained and untrained workers?   \n",
       "\n",
       "                                             answer  \n",
       "0  Cambrian sessile frond-like fossil Stromatoveris  \n",
       "1                     a pharmacy practice residency  \n",
       "2                             period of compression  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# or just load from the csv provided\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/qa_pairs.csv\")\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  arize.utils.logging | WARNING | DEPRECATED: developer_key is deprecated, only api_key is needed.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# get the Arize client\n",
    "# now let's create a dataset in Arize\n",
    "from arize.experimental.datasets import ArizeDatasetsClient\n",
    "from arize.experimental.datasets.utils.constants import GENERATIVE\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "ARIZE_API_KEY = os.getenv(\"ARIZE_API_KEY\")\n",
    "ARIZE_SPACE_ID = os.getenv(\"ARIZE_SPACE_ID\")\n",
    "ARIZE_DEVELOPER_KEY = os.getenv(\"ARIZE_DEVELOPER_KEY\")\n",
    "\n",
    "arize_client = ArizeDatasetsClient(api_key=ARIZE_API_KEY, developer_key=ARIZE_DEVELOPER_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  arize.utils.logging | WARNING | DEPRECATED: developer_key is deprecated, only api_key is needed.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alejandro.cantarero/.pyenv/versions/3.12.10/envs/langflow-dev-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# create the dataset in Arize\n",
    "dataset_id = arize_client.create_dataset(\n",
    "    space_id=ARIZE_SPACE_ID, \n",
    "    dataset_name=\"squad_dataset\",\n",
    "    dataset_type=GENERATIVE,\n",
    "    data=df\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for speed, let's also make a smaller dataset\n",
    "df_small = df.head(20)\n",
    "dataset = arize_client.create_dataset(\n",
    "    data=df_small,\n",
    "    dataset_name=\"squad-dev-v2.0-x-small\",\n",
    "    space_id=ARIZE_SPACE_ID,\n",
    "    dataset_type=GENERATIVE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Langflow Flows from code\n",
    "\n",
    "In this section, we define a method to use the Langflow runtime to execute flows and return the results.\n",
    "\n",
    "You can find details on how to call these methods by clicking the **Publish** button in the upper right corner of the Langflow UI when a flow is open and you are editing it.  Then select **API Access**.\n",
    "\n",
    "The code below assumes you are running Langflow locally on your machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to run a flow in Langflow\n",
    "# trigger flow via API\n",
    "import requests\n",
    "\n",
    "BASE_API_URL = \"http://127.0.0.1:7860\"\n",
    "\n",
    "def run_flow(input: str, flow_id: str, input_type: str = \"chat\", tweaks: dict = None):\n",
    "    \"\"\" Raises exceptions for non-200 status code from langflow response\n",
    "    \"\"\"\n",
    "    api_url = f\"{BASE_API_URL}/api/v1/run/{flow_id}\"\n",
    "\n",
    "    payload = {\n",
    "        \"input_value\": input,\n",
    "        \"output_type\": \"chat\",\n",
    "        \"input_type\": input_type,\n",
    "    }\n",
    "\n",
    "    if tweaks:\n",
    "        payload[\"tweaks\"] = tweaks\n",
    "\n",
    "    timeout = 10\n",
    "    attempts = 6\n",
    "    for i in range(attempts):\n",
    "        try:\n",
    "            response = requests.post(api_url, json=payload, timeout=timeout)\n",
    "            if response.status_code != 200:\n",
    "                raise requests.exceptions.RequestException(f\"Status code: {response.status_code}\")\n",
    "            return response\n",
    "        \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"The flow request timed out. Attempt {i}, current timeout {timeout}.\")\n",
    "            timeout *= 2\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(e)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run an experiment with Langflow and Arize\n",
    "\n",
    "1. Define a task to run the the a flow on the dataset\n",
    "2. Create an LLM to act as a judge\n",
    "3. Define an evaluator to measure the accuracy of the results\n",
    "4. Run the experiment and log the results to Arize Phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_FLOW_ID = \"796625a6-2526-4e12-b2f8-873d66940016\"\n",
    "\n",
    "def task(dataset_row) -> str:\n",
    "    question = dataset_row[\"question\"]\n",
    "    response = run_flow(question, CHAT_FLOW_ID)\n",
    "    text = response.json()['outputs'][0]['outputs'][0]['results']['message'][\"text\"]\n",
    "    return text   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from phoenix.evals.models import OpenAIModel\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "judge_model = OpenAIModel(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "from phoenix.evals import (\n",
    "    QA_PROMPT_RAILS_MAP,\n",
    "    QA_PROMPT_TEMPLATE,\n",
    "    OpenAIModel,\n",
    "    llm_classify,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are given a question, an answer and reference text. You must determine whether the\n",
      "given answer correctly answers the question based on the reference text. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: {input}\n",
      "    ************\n",
      "    [Reference]: {reference}\n",
      "    ************\n",
      "    [Answer]: {output}\n",
      "    [END DATA]\n",
      "Your response must be a single word, either \"correct\" or \"incorrect\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"correct\" means that the question is correctly and fully answered by the answer.\n",
      "\"incorrect\" means that the question is not correctly or only partially answered by the\n",
      "answer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's look at the prompt template\n",
    "print(QA_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter names in evaluation function: d, a, t, a, s, e, t, _, r, o, w. Parameters names for multi-argument functions must be any of: metadata, reference, output, input, expected.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mphoenix\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperiments\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevaluators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_evaluator\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mphoenix\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperiments\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EvaluationResult\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;129;43m@create_evaluator\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAnswer Correctness\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mLLM\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43manswer_correctness\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_row\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[43mEvaluationResult\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_row\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_row\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43manswer\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.10/envs/langflow-dev-env/lib/python3.12/site-packages/phoenix/experiments/evaluators/utils.py:157\u001b[39m, in \u001b[36mcreate_evaluator.<locals>.wrapper\u001b[39m\u001b[34m(func)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    156\u001b[39m wrapped_signature = inspect.signature(func)\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m \u001b[43mvalidate_evaluator_signature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapped_signature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inspect.iscoroutinefunction(func):\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrap_coroutine_evaluation_function(name, kind, wrapped_signature, scorer)(func)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.10/envs/langflow-dev-env/lib/python3.12/site-packages/phoenix/experiments/evaluators/utils.py:44\u001b[39m, in \u001b[36mvalidate_evaluator_signature\u001b[39m\u001b[34m(sig)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m     40\u001b[39m     param.kind \u001b[38;5;129;01mis\u001b[39;00m inspect.Parameter.VAR_KEYWORD\n\u001b[32m     41\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m param.default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inspect.Parameter.empty\n\u001b[32m     42\u001b[39m ):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     45\u001b[39m     (\n\u001b[32m     46\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid parameter names in evaluation function: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(not_found)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     47\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mParameters names for multi-argument functions must be \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     48\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33many of: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(valid_named_params)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     49\u001b[39m     )\n\u001b[32m     50\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: Invalid parameter names in evaluation function: d, a, t, a, s, e, t, _, r, o, w. Parameters names for multi-argument functions must be any of: metadata, reference, output, input, expected."
     ]
    }
   ],
   "source": [
    "from phoenix.experiments.evaluators import create_evaluator\n",
    "from phoenix.experiments.types import EvaluationResult\n",
    "\n",
    "@create_evaluator(name=\"Answer Correctness\", kind=\"LLM\")\n",
    "def answer_correctness(output, dataset_row) -> EvaluationResult:\n",
    "\n",
    "    question = dataset_row[\"question\"]\n",
    "    answer = dataset_row[\"answer\"]\n",
    "    df_in = pd.DataFrame({\n",
    "        \"input\": question,\n",
    "        \"output\": output,\n",
    "        \"reference\": answer,\n",
    "    }, index=[0])\n",
    "              \n",
    "    rails = list(QA_PROMPT_RAILS_MAP.values())\n",
    "    \n",
    "    eval_df = llm_classify(\n",
    "        data=df_in,\n",
    "        template=QA_PROMPT_TEMPLATE,\n",
    "        model=judge_model,\n",
    "        rails=rails,\n",
    "        provide_explanation=True,\n",
    "        run_sync=True,\n",
    "    )\n",
    "\n",
    "    label = eval_df[\"label\"][0]\n",
    "    explanation = eval_df[\"explanation\"][0]\n",
    "    score = 1 if label == \"correct\" else 0\n",
    "\n",
    "    return EvaluationResult(label=label, score=score, explanation=explanation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset IDs - get these from the Arize UI\n",
    "squad_qa_20 = \"RGF0YXNldDozNDMxNjowWFIx\"  \n",
    "squad_qa_100 = \"RGF0YXNldDozNDMxNTpSZ21u\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter names in evaluation function: e, x, p, e, c, t, e, d. Parameters names for multi-argument functions must be any of: dataset_output, metadata, experiment_output, output, input, dataset_row.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43marize_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspace_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mARIZE_SPACE_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43msquad_qa_20\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43manswer_correctness\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mExperiment 3\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexit_on_error\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.10/envs/langflow-dev-env/lib/python3.12/site-packages/arize/experimental/datasets/core/client.py:205\u001b[39m, in \u001b[36mArizeDatasetsClient.run_experiment\u001b[39m\u001b[34m(self, space_id, experiment_name, task, dataset_df, dataset_id, dataset_name, evaluators, dry_run, concurrency, set_global_tracer_provider, exit_on_error)\u001b[39m\n\u001b[32m    194\u001b[39m \u001b[38;5;66;03m# trace model and resource for the experiment\u001b[39;00m\n\u001b[32m    195\u001b[39m tracer, resource = _get_tracer_resource(\n\u001b[32m    196\u001b[39m     model_id=trace_model_name,\n\u001b[32m    197\u001b[39m     space_id=space_id,\n\u001b[32m   (...)\u001b[39m\u001b[32m    202\u001b[39m     set_global_tracer_provider=set_global_tracer_provider,\n\u001b[32m    203\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m output_df = \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtracer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtracer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresource\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconcurrency\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconcurrency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexit_on_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexit_on_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    215\u001b[39m output_df = _convert_default_columns_to_json_str(output_df)\n\u001b[32m    216\u001b[39m output_df = _convert_boolean_columns_to_str(output_df)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.10/envs/langflow-dev-env/lib/python3.12/site-packages/arize/experimental/datasets/experiments/functions.py:97\u001b[39m, in \u001b[36mrun_experiment\u001b[39m\u001b[34m(experiment_name, dataset, task, tracer, resource, rate_limit_errors, evaluators, concurrency, exit_on_error)\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m examples:\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo examples found in the dataset.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m evaluators_by_name = \u001b[43m_evaluators_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m root_span_name = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTask: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mget_func_name(task)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     99\u001b[39m root_span_kind = CHAIN\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.10/envs/langflow-dev-env/lib/python3.12/site-packages/arize/experimental/datasets/experiments/functions.py:660\u001b[39m, in \u001b[36m_evaluators_by_name\u001b[39m\u001b[34m(obj)\u001b[39m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, Sequence):\n\u001b[32m    658\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m obj:\n\u001b[32m    659\u001b[39m         evaluator = (\n\u001b[32m--> \u001b[39m\u001b[32m660\u001b[39m             \u001b[43mcreate_evaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    661\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Evaluator)\n\u001b[32m    662\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m value\n\u001b[32m    663\u001b[39m         )\n\u001b[32m    664\u001b[39m         name = evaluator.name\n\u001b[32m    665\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m evaluators_by_name:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.10/envs/langflow-dev-env/lib/python3.12/site-packages/arize/experimental/datasets/experiments/evaluators/utils.py:96\u001b[39m, in \u001b[36mcreate_evaluator.<locals>.wrapper\u001b[39m\u001b[34m(func)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     95\u001b[39m wrapped_signature = inspect.signature(func)\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m \u001b[43mvalidate_evaluator_signature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapped_signature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inspect.iscoroutinefunction(func):\n\u001b[32m     99\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrap_coroutine_evaluation_function(\n\u001b[32m    100\u001b[39m         name, wrapped_signature, scorer\n\u001b[32m    101\u001b[39m     )(func)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.10/envs/langflow-dev-env/lib/python3.12/site-packages/arize/experimental/datasets/experiments/evaluators/utils.py:49\u001b[39m, in \u001b[36mvalidate_evaluator_signature\u001b[39m\u001b[34m(sig)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m     45\u001b[39m     param.kind \u001b[38;5;129;01mis\u001b[39;00m inspect.Parameter.VAR_KEYWORD\n\u001b[32m     46\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m param.default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inspect.Parameter.empty\n\u001b[32m     47\u001b[39m ):\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     50\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid parameter names in evaluation function: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(not_found)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     51\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mParameters names for multi-argument functions must be \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     52\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33many of: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(valid_named_params)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     53\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: Invalid parameter names in evaluation function: e, x, p, e, c, t, e, d. Parameters names for multi-argument functions must be any of: dataset_output, metadata, experiment_output, output, input, dataset_row."
     ]
    }
   ],
   "source": [
    "arize_client.run_experiment(\n",
    "    space_id=ARIZE_SPACE_ID,\n",
    "    dataset_id=squad_qa_20,\n",
    "    task=task,\n",
    "    evaluators=[answer_correctness],\n",
    "    experiment_name=\"Experiment 3\",\n",
    "    exit_on_error=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langflow-dev-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
