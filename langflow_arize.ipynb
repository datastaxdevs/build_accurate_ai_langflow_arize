{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure accuracy of agentic app with Arize and Langflow\n",
    "\n",
    "In this notebook we'll step through how to use open source tools [Arize](https://arize.com/) and [Langflow](https://www.langflow.org/) and to measure the accuracy of an agentic application.\n",
    "\n",
    "## Prepping a Dataset\n",
    "\n",
    "Let's start by gathering some example data from a standard question and answer benchmark. We'll use the Standford Question Answering Dataset (SQuAD).\n",
    "\n",
    "Available at [https://rajpurkar.github.io/SQuAD-explorer/](https://rajpurkar.github.io/SQuAD-explorer/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully\n",
      "Dataset loaded with 35 articles\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Download the file\n",
    "url = 'https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the download was successful\n",
    "if response.status_code == 200:\n",
    "    # Save the file locally\n",
    "    with open('dev-v2.0.json', 'w') as file:\n",
    "        json.dump(response.json(), file)\n",
    "    print(\"File downloaded successfully\")\n",
    "    \n",
    "    # Load the data\n",
    "    data = response.json()\n",
    "    print(f\"Dataset loaded with {len(data['data'])} articles\")\n",
    "else:\n",
    "    print(f\"Failed to download file. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all articles from the dataset\n",
    "docs = []\n",
    "for record in data[\"data\"]:\n",
    "    text = \"\"\n",
    "    for paragraph in record[\"paragraphs\"]:\n",
    "        text += paragraph[\"context\"] + \"\\n\"\n",
    "    docs.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of wikipedia articles in dataset: 35.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of wikipedia articles in dataset: {len(docs)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create markdown files for each article for use in Langflow UI\n",
    "for i, doc in enumerate(docs):\n",
    "    filename = f\"data/markdown_files/article_{i}.md\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all questions and answers for each article\n",
    "qandas = []\n",
    "for record in data[\"data\"]:\n",
    "    for paragraph in record[\"paragraphs\"]:\n",
    "        for qa in paragraph[\"qas\"]:\n",
    "            if len(qa[\"answers\"]) > 0:\n",
    "                qandas.append((qa[\"question\"], qa[\"answers\"][0][\"text\"]))\n",
    "\n",
    "# there are about 6000 Q&A pairs in this data set.  \n",
    "# Let's shorten to 100 to make it easier to run our tests\n",
    "import random\n",
    "samples = random.sample(qandas, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('What type of fossils were found in China?', 'Cambrian sessile frond-like fossil Stromatoveris')\n",
      "('Where do pharmacists acquire more preparation following pharmacy school?', 'a pharmacy practice residency')\n",
      "('What contributed to the decreased inequality between trained and untrained workers?', 'period of compression')\n"
     ]
    }
   ],
   "source": [
    "# Let's look at a couple examples of questions and answers\n",
    "print(samples[0])\n",
    "print(samples[1])\n",
    "print(samples[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install modules needed for the rest of this example\n",
    "\n",
    "We'll need the following modules to be available\n",
    "\n",
    "- `langflow` for rapidly designing AI workflows\n",
    "- `phoenix-arize` to run evals\n",
    "- `pandas` to format the data\n",
    "- `openai` for access to LLMs and embedding models\n",
    "\n",
    "If you don't already have these installed, go ahead and run the install commands below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: uv in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.5.15)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPython 3.11.7 interpreter at: \u001b[36m/opt/homebrew/opt/python@3.11/bin/python3.11\u001b[39m\n",
      "Creating virtual environment at: \u001b[36m.venv\u001b[39m\n",
      "Activate with: \u001b[32msource .venv/bin/activate\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "# uv requires a virtual environment to run\n",
    "!uv venv\n",
    "!source .venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install all the modules for the rest of the notebook\n",
    "!uv pip install langflow pandas openai arize -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our ground truth data to Arize\n",
    "\n",
    "Now that we have a set of questions and answers, we can use this to evaluate the quality of our agentic application.\n",
    "\n",
    "Let's upload this dataset to Phoenix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start by creating a dataframe with our Q&A pairs and save a copy\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(samples, columns=[\"question\", \"answer\"])\n",
    "\n",
    "# only run this line if you want to resave the dataframe\n",
    "df.to_csv(\"data/qa_pairs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What type of fossils were found in China?</td>\n",
       "      <td>Cambrian sessile frond-like fossil Stromatoveris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Where do pharmacists acquire more preparation following pharmacy school?</td>\n",
       "      <td>a pharmacy practice residency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What contributed to the decreased inequality between trained and untrained workers?</td>\n",
       "      <td>period of compression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                              question  \\\n",
       "0                                            What type of fossils were found in China?   \n",
       "1             Where do pharmacists acquire more preparation following pharmacy school?   \n",
       "2  What contributed to the decreased inequality between trained and untrained workers?   \n",
       "\n",
       "                                             answer  \n",
       "0  Cambrian sessile frond-like fossil Stromatoveris  \n",
       "1                     a pharmacy practice residency  \n",
       "2                             period of compression  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# or just load from the csv provided\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/qa_pairs.csv\")\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  arize.utils.logging | WARNING | DEPRECATED: developer_key is deprecated, only api_key is needed.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# get the Arize client\n",
    "# now let's create a dataset in Arize\n",
    "from arize.experimental.datasets import ArizeDatasetsClient\n",
    "from arize.experimental.datasets.utils.constants import GENERATIVE\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "ARIZE_API_KEY = os.getenv(\"ARIZE_API_KEY\")\n",
    "ARIZE_SPACE_ID = os.getenv(\"ARIZE_SPACE_ID\")\n",
    "ARIZE_DEVELOPER_KEY = os.getenv(\"ARIZE_DEVELOPER_KEY\")\n",
    "\n",
    "arize_client = ArizeDatasetsClient(api_key=ARIZE_API_KEY, developer_key=ARIZE_DEVELOPER_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  arize.utils.logging | WARNING | DEPRECATED: developer_key is deprecated, only api_key is needed.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alejandro.cantarero/.pyenv/versions/3.12.10/envs/langflow-dev-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# create the dataset in Arize\n",
    "dataset_id = arize_client.create_dataset(\n",
    "    space_id=ARIZE_SPACE_ID, \n",
    "    dataset_name=\"squad_dataset\",\n",
    "    dataset_type=GENERATIVE,\n",
    "    data=df\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for speed, let's also make a smaller dataset\n",
    "df_small = df.head(20)\n",
    "dataset = arize_client.create_dataset(\n",
    "    data=df_small,\n",
    "    dataset_name=\"squad-dev-v2.0-x-small\",\n",
    "    space_id=ARIZE_SPACE_ID,\n",
    "    dataset_type=GENERATIVE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Langflow Flows from code\n",
    "\n",
    "In this section, we define a method to use the Langflow runtime to execute flows and return the results.\n",
    "\n",
    "You can find details on how to call these methods by clicking the **Publish** button in the upper right corner of the Langflow UI when a flow is open and you are editing it.  Then select **API Access**.\n",
    "\n",
    "The code below assumes you are running Langflow locally on your machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to run a flow in Langflow\n",
    "# trigger flow via API\n",
    "import requests\n",
    "\n",
    "BASE_API_URL = \"http://127.0.0.1:7860\"\n",
    "\n",
    "def run_flow(input: str, flow_id: str, input_type: str = \"chat\", tweaks: dict = None):\n",
    "    \"\"\" Raises exceptions for non-200 status code from langflow response\n",
    "    \"\"\"\n",
    "    api_url = f\"{BASE_API_URL}/api/v1/run/{flow_id}\"\n",
    "\n",
    "    payload = {\n",
    "        \"input_value\": input,\n",
    "        \"output_type\": \"chat\",\n",
    "        \"input_type\": input_type,\n",
    "    }\n",
    "\n",
    "    if tweaks:\n",
    "        payload[\"tweaks\"] = tweaks\n",
    "\n",
    "    timeout = 25\n",
    "    attempts = 6\n",
    "    for i in range(attempts):\n",
    "        try:\n",
    "            response = requests.post(api_url, json=payload, timeout=timeout)\n",
    "            if response.status_code != 200:\n",
    "                raise requests.exceptions.RequestException(f\"Status code: {response.status_code}\")\n",
    "            return response\n",
    "        \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"The flow request timed out. Attempt {i}, current timeout {timeout}.\")\n",
    "            timeout *= 2\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(e)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run an experiment with Langflow and Arize\n",
    "\n",
    "1. Define a task to run the the a flow on the dataset\n",
    "2. Create an LLM to act as a judge\n",
    "3. Define an evaluator to measure the accuracy of the results\n",
    "4. Run the experiment and log the results to Arize Phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_FLOW_ID = \"796625a6-2526-4e12-b2f8-873d66940016\"\n",
    "\n",
    "def task(dataset_row) -> str:\n",
    "    question = dataset_row[\"question\"]\n",
    "    response = run_flow(question, CHAT_FLOW_ID)\n",
    "    text = response.json()['outputs'][0]['outputs'][0]['results']['message'][\"text\"]\n",
    "    return text   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from phoenix.evals.models import OpenAIModel\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "judge_model = OpenAIModel(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "from phoenix.evals import (\n",
    "    QA_PROMPT_RAILS_MAP,\n",
    "    QA_PROMPT_TEMPLATE,\n",
    "    OpenAIModel,\n",
    "    llm_classify,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are given a question, an answer and reference text. You must determine whether the\n",
      "given answer correctly answers the question based on the reference text. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: {input}\n",
      "    ************\n",
      "    [Reference]: {reference}\n",
      "    ************\n",
      "    [Answer]: {output}\n",
      "    [END DATA]\n",
      "Your response must be a single word, either \"correct\" or \"incorrect\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"correct\" means that the question is correctly and fully answered by the answer.\n",
      "\"incorrect\" means that the question is not correctly or only partially answered by the\n",
      "answer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's look at the prompt template\n",
    "print(QA_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from phoenix.experiments.evaluators import create_evaluator\n",
    "from arize.experimental.datasets.experiments.evaluators.base import EvaluationResult\n",
    "import pandas as pd\n",
    "\n",
    "#@create_evaluator(name=\"Answer Correctness\", kind=\"LLM\")\n",
    "def answer_correctness(input, output, dataset_row) -> EvaluationResult:\n",
    "\n",
    "    question = dataset_row[\"question\"]\n",
    "    answer = dataset_row[\"answer\"]\n",
    "    df_in = pd.DataFrame({\n",
    "        \"input\": question,\n",
    "        \"output\": output,\n",
    "        \"reference\": answer,\n",
    "    }, index=[0])\n",
    "              \n",
    "    rails = list(QA_PROMPT_RAILS_MAP.values())\n",
    "    \n",
    "    eval_df = llm_classify(\n",
    "        data=df_in,\n",
    "        template=QA_PROMPT_TEMPLATE,\n",
    "        model=judge_model,\n",
    "        rails=rails,\n",
    "        provide_explanation=True,\n",
    "        run_sync=True,\n",
    "    )\n",
    "    \n",
    "    label = eval_df[\"label\"][0]\n",
    "    explanation = eval_df[\"explanation\"][0]\n",
    "    score = 1 if label == \"correct\" else 0\n",
    "\n",
    "    return EvaluationResult(label=label, score=score, explanation=explanation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset IDs - get these from the Arize UI\n",
    "squad_qa_20 = \"RGF0YXNldDozNDMxNjowWFIx\"  \n",
    "squad_qa_100 = \"RGF0YXNldDozNDMxNTpSZ21u\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üêå!! If running inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n",
      "running tasks |          | 0/100 (0.0%) | ‚è≥ 00:00<? | ?it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The flow request timed out. Attempt 0, current timeout 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |‚ñà‚ñà‚ñç       | 24/100 (24.0%) | ‚è≥ 02:33<07:13 |  5.70s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The flow request timed out. Attempt 0, current timeout 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |‚ñà‚ñà‚ñà       | 31/100 (31.0%) | ‚è≥ 03:24<06:16 |  5.45s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The flow request timed out. Attempt 0, current timeout 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |‚ñà‚ñà‚ñà‚ñã      | 37/100 (37.0%) | ‚è≥ 04:11<07:18 |  6.96s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The flow request timed out. Attempt 0, current timeout 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñå     | 46/100 (46.0%) | ‚è≥ 05:22<05:35 |  6.22s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The flow request timed out. Attempt 0, current timeout 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/100 (48.0%) | ‚è≥ 05:53<08:58 | 10.35s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The flow request timed out. Attempt 0, current timeout 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/100 (56.0%) | ‚è≥ 07:04<06:06 |  8.32s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The flow request timed out. Attempt 0, current timeout 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 (57.0%) | ‚è≥ 07:25<08:37 | 12.03s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The flow request timed out. Attempt 0, current timeout 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 64/100 (64.0%) | ‚è≥ 08:24<05:02 |  8.39s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The flow request timed out. Attempt 0, current timeout 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 (81.0%) | ‚è≥ 10:18<01:51 |  5.86s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The flow request timed out. Attempt 0, current timeout 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 82/100 (82.0%) | ‚è≥ 10:39<03:06 | 10.34s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The flow request timed out. Attempt 0, current timeout 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 90/100 (90.0%) | ‚è≥ 11:41<01:06 |  6.69s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The flow request timed out. Attempt 0, current timeout 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 93/100 (93.0%) | ‚è≥ 12:11<00:54 |  7.81s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The flow request timed out. Attempt 0, current timeout 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 (100.0%) | ‚è≥ 13:08<00:00 |  6.67s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (04/28/25 08:23 PM -0700)\n",
      "---------------------------------------\n",
      "|   n_examples |   n_runs |   n_errors |\n",
      "|-------------:|---------:|-----------:|\n",
      "|          100 |      100 |          0 |\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üêå!! If running inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n",
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 (100.0%) | ‚è≥ 13:08<00:00 |  7.88s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.04s/it<? | ?it/s\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.40s/it<03:30 |  2.12s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.90s/it<03:49 |  2.34s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:04<00:00 |  4.01s/it<04:15 |  2.64s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:07<00:00 |  7.80s/it<05:08 |  3.21s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.95s/it<07:45 |  4.90s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.40s/it<06:40 |  4.27s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:03<00:00 |  3.48s/it<05:42 |  3.69s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:03<00:00 |  3.26s/it<05:35 |  3.65s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:03<00:00 |  3.03s/it<05:23 |  3.55s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:04<00:00 |  4.01s/it35<05:07 |  3.42s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:03<00:00 |  3.66s/it39<05:22 |  3.63s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:03<00:00 |  3.32s/it43<05:22 |  3.67s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:03<00:00 |  3.17s/it46<05:12 |  3.59s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:01<00:00 |  1.82s/it49<05:00 |  3.49s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.73s/it51<04:16 |  3.02s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.51s/it54<04:08 |  2.96s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.79s/it57<03:56 |  2.85s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.50s/it59<03:54 |  2.86s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:01<00:00 |  1.79s/it02<03:45 |  2.78s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.53s/it04<03:20 |  2.51s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.04s/it06<03:20 |  2.54s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.92s/it09<03:08 |  2.42s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:03<00:00 |  3.58s/it12<03:20 |  2.60s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:05<00:00 |  5.49s/it15<03:41 |  2.92s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.53s/it21<04:38 |  3.71s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.27s/it24<04:10 |  3.39s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.13s/it26<03:44 |  3.08s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.43s/it28<03:23 |  2.82s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.09s/it31<03:14 |  2.73s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:03<00:00 |  3.62s/it33<02:59 |  2.57s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:01<00:00 |  1.66s/it36<03:20 |  2.91s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.28s/it38<02:54 |  2.56s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.97s/it41<02:47 |  2.51s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:03<00:00 |  3.98s/it44<02:56 |  2.67s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 01:55<00:00 | 115.66s/it8<03:21 |  3.09s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.54s/it44<39:21 | 36.89s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.11s/it46<27:56 | 26.61s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.06s/it48<19:55 | 19.29s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:03<00:00 |  3.05s/it51<14:23 | 14.15s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.45s/it54<10:50 | 10.85s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:03<00:00 |  3.45s/it56<08:12 |  8.35s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.54s/it00<06:40 |  6.91s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:03<00:00 |  3.22s/it02<05:20 |  5.62s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:03<00:00 |  3.40s/it06<04:37 |  4.96s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.18s/it09<04:10 |  4.55s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:08<00:00 |  8.38s/it12<03:29 |  3.87s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.24s/it20<04:38 |  5.26s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:01<00:00 |  1.91s/it22<03:47 |  4.38s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:03<00:00 |  3.31s/it24<03:06 |  3.66s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.03s/it28<02:59 |  3.58s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.82s/it30<02:34 |  3.15s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.84s/it33<02:27 |  3.07s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:01<00:00 |  1.87s/it36<02:22 |  3.03s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.74s/it38<02:04 |  2.71s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:04<00:00 |  4.13s/it41<02:03 |  2.75s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.85s/it45<02:20 |  3.19s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:01<00:00 |  1.84s/it48<02:14 |  3.12s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:01<00:00 |  1.80s/it50<01:56 |  2.76s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:01<00:00 |  1.97s/it52<01:42 |  2.50s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:01<00:00 |  1.89s/it54<01:34 |  2.37s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.73s/it56<01:27 |  2.25s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.62s/it58<01:32 |  2.42s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:01<00:00 |  1.65s/it01<01:32 |  2.51s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:01<00:00 |  1.89s/it03<01:22 |  2.28s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.72s/it05<01:16 |  2.19s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.51s/it08<01:20 |  2.37s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.19s/it10<01:20 |  2.44s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.83s/it13<01:16 |  2.39s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.14s/it16<01:19 |  2.55s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:05<00:00 |  5.99s/it18<01:13 |  2.45s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.49s/it24<01:42 |  3.54s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:01<00:00 |  1.57s/it27<01:32 |  3.30s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.65s/it28<01:15 |  2.81s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:03<00:00 |  3.12s/it31<01:12 |  2.79s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:01<00:00 |  1.82s/it34<01:12 |  2.91s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:01<00:00 |  1.69s/it36<01:02 |  2.61s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:01<00:00 |  1.78s/it38<00:54 |  2.36s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:01<00:00 |  1.95s/it40<00:48 |  2.21s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:01<00:00 |  1.86s/it42<00:45 |  2.16s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:03<00:00 |  3.28s/it44<00:41 |  2.10s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:04<00:00 |  4.43s/it47<00:47 |  2.48s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:03<00:00 |  3.47s/it52<00:55 |  3.09s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.86s/it55<00:54 |  3.23s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.91s/it58<00:50 |  3.14s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.04s/it01<00:46 |  3.10s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:01<00:00 |  1.80s/it03<00:39 |  2.81s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.08s/it05<00:32 |  2.53s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:01<00:00 |  1.75s/it07<00:29 |  2.42s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:03<00:00 |  3.23s/it09<00:24 |  2.25s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.22s/it12<00:25 |  2.57s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:01<00:00 |  1.86s/it15<00:22 |  2.49s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:01<00:00 |  1.65s/it17<00:18 |  2.33s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:03<00:00 |  3.54s/it18<00:15 |  2.15s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:01<00:00 |  1.97s/it22<00:15 |  2.59s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.47s/it24<00:12 |  2.44s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.97s/it27<00:09 |  2.48s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:01<00:00 |  1.71s/it30<00:07 |  2.65s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.21s/it32<00:04 |  2.39s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.73s/it34<00:02 |  2.37s/it\n",
      "running experiment evaluations |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 (100.0%) | ‚è≥ 06:37<00:00 |  3.97s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('RXhwZXJpbWVudDo5OTA1OlB3MlA=',\n",
       "                id                            example_id  \\\n",
       " 0   EXP_ID_05d1f6  fd618731-c84c-4b19-a3cc-1926cfaf46e9   \n",
       " 1   EXP_ID_e17228  cb30a0b3-25fc-4e02-83ef-d24e0854bfe0   \n",
       " 2   EXP_ID_48e019  652d1890-e423-48d4-82fb-f6b81aec92d2   \n",
       " 3   EXP_ID_51c6fd  ca6ebe6d-14b8-4636-9248-73bc5e600a2a   \n",
       " 4   EXP_ID_919634  e210d864-6a21-4988-800e-6bd2b584dcc8   \n",
       " ..            ...                                   ...   \n",
       " 95  EXP_ID_c8f322  5d3c5fac-d7fd-4b4d-a955-1fc48636906e   \n",
       " 96  EXP_ID_46f4a8  4f309848-a47c-410b-9ba1-eaa387d442d2   \n",
       " 97  EXP_ID_4f2119  b6ba039c-28df-400a-9884-93b20236a00f   \n",
       " 98  EXP_ID_cf8489  3ae1c206-29d2-4886-acc1-e0d5dd4be250   \n",
       " 99  EXP_ID_873646  92038ae5-482d-4d90-9906-76d0a0da015c   \n",
       " \n",
       "                                                result  \\\n",
       " 0   In China, significant fossils have been discov...   \n",
       " 1   Pharmacists can acquire additional preparation...   \n",
       " 2   The decreased inequality between trained and u...   \n",
       " 3   In South Carolina, Huguenot nobility primarily...   \n",
       " 4   It is now possible to convert old relative age...   \n",
       " ..                                                ...   \n",
       " 95  One type of computing method used to find prim...   \n",
       " 96  The first Huguenot to arrive at the Cape of Go...   \n",
       " 97           I don't have access to that information.   \n",
       " 98  In 2008, the percentage of German students att...   \n",
       " 99  The vast majority of the population is typical...   \n",
       " \n",
       "                      result.trace.id  result.trace.timestamp  \\\n",
       " 0   9a349aa8e2286a62eb1931111e5b17b6           1745896228982   \n",
       " 1   ed0249d01be3e278644f2d4c5a6e8440           1745896248953   \n",
       " 2   b7554d1caef4618094f739ab697ef3f0           1745896255008   \n",
       " 3   505015483fe9f5f13644e2de6ebf2491           1745896260638   \n",
       " 4   af94179e7d6f47ecbe01fbe4ba0479ab           1745896268040   \n",
       " ..                               ...                     ...   \n",
       " 95  a2f5108c3e4947f319cad7783ffe70b6           1745896985598   \n",
       " 96  28047f5278367dfb6bff7048fb7c53ef           1745896990316   \n",
       " 97  7f0c6bfad8042d8db10e734d43559740           1745896998876   \n",
       " 98  94d998eb6e18e1a7768ae09b34212fde           1745897002142   \n",
       " 99  ae0ca5e547f42182dda022b66e78289e           1745897011098   \n",
       " \n",
       "     eval.answer_correctness.score eval.answer_correctness.label  \\\n",
       " 0                               1                       correct   \n",
       " 1                               1                       correct   \n",
       " 2                               1                       correct   \n",
       " 3                               1                       correct   \n",
       " 4                               1                       correct   \n",
       " ..                            ...                           ...   \n",
       " 95                              0                     incorrect   \n",
       " 96                              1                       correct   \n",
       " 97                              0                     incorrect   \n",
       " 98                              1                       correct   \n",
       " 99                              1                       correct   \n",
       " \n",
       "                   eval.answer_correctness.explanation  \\\n",
       " 0   The question asks specifically about the type ...   \n",
       " 1   To determine if the answer is correct, we firs...   \n",
       " 2   To determine if the answer correctly addresses...   \n",
       " 3   The question asks where in South Carolina Hugu...   \n",
       " 4   To determine if the answer is correct, we firs...   \n",
       " ..                                                ...   \n",
       " 95  The question asks for the name of one type of ...   \n",
       " 96  To determine if the answer is correct, we firs...   \n",
       " 97  The question asks for the cause of devastation...   \n",
       " 98  To determine if the answer is correct, we firs...   \n",
       " 99  The question asks about the type of income tha...   \n",
       " \n",
       "     eval.answer_correctness.trace.id  eval.answer_correctness.trace.timestamp  \n",
       " 0   f77a3869e1801cadf850f778274f1ddd                            1745897016014  \n",
       " 1   2e7648816576d5d77e30fd64f38b06bc                            1745897018138  \n",
       " 2   95d2e99910874dafcb6e49f2b9230332                            1745897020634  \n",
       " 3   726acd44f9d2d5ea2c598fcc42e438d4                            1745897023619  \n",
       " 4   a0532666a1b0566bb3d1c00a11dc9b39                            1745897027710  \n",
       " ..                               ...                                      ...  \n",
       " 95  4fba12935d389752d2f20044f6946216                            1745897400635  \n",
       " 96  35ef628148e78adc57a2e9f219f6a773                            1745897403197  \n",
       " 97  7c379240cd23458297b01df0d58d0253                            1745897406252  \n",
       " 98  d1149b575f10ba70dd90efd94d913915                            1745897408047  \n",
       " 99  8b415bd437a2635344fe9cc788295972                            1745897410356  \n",
       " \n",
       " [100 rows x 10 columns])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arize_client.run_experiment(\n",
    "    space_id=ARIZE_SPACE_ID,\n",
    "    dataset_id=squad_qa_100,\n",
    "    task=task,\n",
    "    evaluators=[answer_correctness],\n",
    "    experiment_name=\"astra_v3_small_n4_gpt4o_try3\",\n",
    "    exit_on_error=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langflow-dev-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
